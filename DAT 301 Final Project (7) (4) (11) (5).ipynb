{"cells":[{"metadata":{"trusted":false},"cell_type":"markdown","source":"# DAT 301 - Final Exam\n## Sebastian FSV\n## 4/28/2021\n"},{"metadata":{},"cell_type":"markdown","source":"Data scraped from Wikileaks include all 107 emails that contained either the words (favor & confidential) or (favor and classified). Millions of emails belonging to the presidential candidate were made public prior to the 2016 election and contain many of Hillary's personal and professional communications during and leading up to her presidential campaign. \n\nEmails were made public under the aligation that there was nefarious activity that could be infered or directly proved contained within. All of the text that is contained within each email is collected including email addresses and times. The data is stripped and split and turned into a list.\n\nAn equal set of the most common words contained within this email query is compared to the most common words used in George Orwell's 1984. These sets are then studied for a difference in mean usage of their most common words. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# !pip install bs4\n# !pip install requests\n# !pip install pandas\n# !pip install wordcloud\n# !pip install seaborn\n\n# import sys\n# from os import path\n\nimport numpy as np\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup as bs\nfrom wordcloud import WordCloud, STOPWORDS\nimport re\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom random import sample\nimport scipy.stats as ss","execution_count":null,"outputs":[]},{"metadata":{},"id":"liquid-forest","cell_type":"markdown","source":"# Hillary Clinton's Emails"},{"metadata":{"trusted":true},"id":"demonstrated-thanksgiving","cell_type":"code","source":"url = \"https://wikileaks.org/clinton-emails/?q=%28favor+%26+confidential%29+%7C+%28favor+%26+classified%29&mfrom=&mto=&title=&notitle=&date_from=&date_to=&nofrom=&noto=&count=200&sort=0&page=1&#searchresult\"\nr = requests.get(url)\nsoup = bs(r.content, 'html.parser')","execution_count":null,"outputs":[]},{"metadata":{},"id":"inside-austin","cell_type":"markdown","source":"### Locate and Create Table of Target Emails"},{"metadata":{},"cell_type":"markdown","source":"Had to identify uniue table class in order to pull correct table rows into table data, then coerced the table into a data frame."},{"metadata":{"trusted":true},"id":"tracked-reality","cell_type":"code","source":"table = soup.find(\"table\",  class_='table table-striped search-result')\ntab_rows = table.find_all('tr')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"adverse-geography","cell_type":"code","source":"l = []\nfor tr in tab_rows:\n    td = tr.find_all('td')\n    row = tr.get_text().strip() \n    l.append(row)","execution_count":null,"outputs":[]},{"metadata":{},"id":"collaborative-acoustic","cell_type":"markdown","source":"### Turn Table into Data Frame"},{"metadata":{"trusted":true},"id":"ultimate-sword","cell_type":"code","source":"df = pd.DataFrame(l)\ndf[['DocID', 'Date', 'Subject', 'From', 'To']] = df[0].str.split('\\n', expand=True)\ndf = df.iloc[1:,1:]","execution_count":null,"outputs":[]},{"metadata":{},"id":"rolled-enhancement","cell_type":"markdown","source":"### Create Giant Text List"},{"metadata":{},"cell_type":"markdown","source":"#### !!!! THIS CELL TAKES THE LONGEST TO RUN !!!!"},{"metadata":{},"cell_type":"markdown","source":"This `for` loop pulls the entire email contents for each webpage associated to a particular emailid. Changing the row parameters from the first 40 emails to over 100 drastically increases the program's run time, but will successfully scrape 100+ webpages in less than 2 minutes. Creates long list where each element is the contents of an email as a string."},{"metadata":{"trusted":true},"id":"baking-shell","cell_type":"code","source":"url1 = 'https://wikileaks.org/clinton-emails/emailid/'\n\nli = []\nfor i in df.iloc[1:40,0]:\n    one = url1 + i\n    one1 = requests.get(one)\n    two = bs(one1.content, 'html.parser')\n    three = two.find(id='uniquer')\n    four = three.get_text().strip()\n    li.append(four)","execution_count":null,"outputs":[]},{"metadata":{},"id":"retired-portfolio","cell_type":"markdown","source":"### Create Word Usage Data Frame"},{"metadata":{},"cell_type":"markdown","source":"The punctuation marks must be removed and duplicates of words are eliminated using the `punctuation` attribute from the `string` package and the lower() method. The string is then split into individual words and the frequency of each word is counted. A dictionary is made from the Word and Frequency lists and then again turned into a pandas data frame. Once in a data frame, the values are sorted in descending order based on the Count. "},{"metadata":{"trusted":true},"id":"several-saturday","cell_type":"code","source":"wordlist = list(re.sub('[' + string.punctuation + ']', '', four).lower().split())   \nfreq = [wordlist.count(w) for w in wordlist]\nfive = dict(list(zip(wordlist, freq)))\nusage = pd.DataFrame(list(five.items()),columns=['Word', 'Count'])\nusage.sort_values(by='Count', ascending=False, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"id":"exterior-kernel","cell_type":"markdown","source":"# George Orwell's 1984"},{"metadata":{"trusted":true},"id":"architectural-financing","cell_type":"code","source":"url2 = 'http://www.george-orwell.org/1984/'","execution_count":null,"outputs":[]},{"metadata":{},"id":"offshore-cleaning","cell_type":"markdown","source":"### Create Giant Text List"},{"metadata":{"trusted":true},"id":"nutritional-command","cell_type":"code","source":"story = []\nfor i in range(22,23):\n    myurl = url2 + str(i) + '.html'\n    s = requests.get(myurl)\n    ch = bs(s.content, 'html.parser')\n    six = [sib.get_text() for sib in ch.find('h2').next_siblings]\n    story.append(six)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"after-attention","cell_type":"code","source":"eight = [str(i).split() for i in story]\nnine = []\nfor phrase in eight:\n    for word in phrase:\n        nine.append(word)\n","execution_count":null,"outputs":[]},{"metadata":{},"id":"eastern-formation","cell_type":"markdown","source":"### Create Word Usage Data Frame"},{"metadata":{"trusted":true},"id":"cooked-degree","cell_type":"code","source":"words2 = [ str(w).lower().strip() for w in nine ]\ntable = str.maketrans(dict.fromkeys(string.punctuation))\nwords1 = [i.translate(table) for i in nine]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"veterinary-hanging","cell_type":"code","source":"freq1 = [words1.count(w) for w in words1]\nsev = dict(list(zip(words1, freq1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"close-guarantee","cell_type":"code","source":"usage1 = pd.DataFrame(list(sev.items()),columns=['Word', 'Count'])\nusage1.sort_values(by='Count', ascending=False, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{},"id":"decreased-salmon","cell_type":"markdown","source":"## STOPWORDS and Other Filters"},{"metadata":{"trusted":true},"id":"hawaiian-encoding","cell_type":"code","source":"stopwords = []\nstopwords = list(set(STOPWORDS))\nstopwords += ['>','from:','to:', 'no.', 'date:','sent:','subject:', 're:',\n             'original', 'message', 'cameron', 'robinson', 'shaun',\n             'c05774510', '11302015', 'f201420439', \"o\\\\'brien\", \"Winston\", 'There']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"desirable-community","cell_type":"code","source":"filt = (usage.Word.isin(stopwords))\nfilt1 = (usage1.Word.isin(stopwords))\nunique = usage[~filt]\nunique1 = usage1[~filt1]\nlength = [len(w) for w in unique.Word]\nlength1 = [len(w) for w in unique1.Word]\nunique.insert(2, 'Length', length, True)\nunique1.insert(2, 'Length', length1, True)\nfilt2 = (unique['Length'] > 4)\nfilt3 = (unique1['Length'] > 4)\n#---------------------------------------------------\nusage2 = usage.sort_values(by='Count', ascending=True)\nusage3 = usage1.sort_values(by='Count', ascending=True)\nfilt6 = (usage2.Word.isin(stopwords))\nfilt7 = (usage3.Word.isin(stopwords))\nunique2 = usage2[~filt6]\nunique3 = usage3[~filt7]\n\nlength2 = [len(w) for w in unique2.Word]\nlength3 = [len(w) for w in unique3.Word]\nunique2.insert(2, 'Length', length, True)\nunique3.insert(2, 'Length', length1, True)\nfilt4 = (unique2['Length'] > 4)\nfilt5 = (unique3['Length'] > 4)","execution_count":null,"outputs":[]},{"metadata":{},"id":"grand-compact","cell_type":"markdown","source":"### Final Data Frames and Lists"},{"metadata":{},"cell_type":"markdown","source":"Being a deliborate novel, the deluge of words from 1984 more than quadrupled the amount of words made availabe for analysis when compared to Mrs. Clinton's emails. Therefore, a length check had to added to ensure a similar proportion of words are being considered from both groups.\n\nDepending on the number of words imported from Hillary's emails, a randomly selected set of the same size is chosen from the larger set of 1984 words.\n\nNothing is hard coded and this entire program is fully scalable. "},{"metadata":{"trusted":true},"id":"excess-sewing","cell_type":"code","source":"a=unique[filt2].iloc[1:11,0:2]\nb=unique1[filt3].iloc[1:11,0:2]\n#--------------\nc=unique2[filt4].iloc[1:11,0:2]\nd=unique3[filt5].iloc[1:11,0:2]\n\nhrc_lst = unique[filt2]\n_1984_lst = unique1[filt3]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"official-hungarian","cell_type":"code","source":"e = unique[filt2]\n\ne['Type'] = np.repeat('HRC', len(e['Word']))\nff = unique1[filt3]\nff['Type'] = np.repeat('1984', len(ff['Word']))\nfff = sample(list(np.arange(0,len(ff['Word']), step=1)), len(e['Word']))\n\nf = ff.iloc[fff]","execution_count":null,"outputs":[]},{"metadata":{},"id":"supported-english","cell_type":"markdown","source":"### Length Checks"},{"metadata":{"trusted":true},"id":"advanced-array","cell_type":"code","source":"mydict = {\n    'HRC'  : [len(hrc_lst['Word'])],\n    '1984' : [len(_1984_lst['Word'])]}\n\ndf2 = pd.DataFrame(mydict, index=['words'])\ndf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"nutritional-fields","cell_type":"code","source":"hrc = e['Word'].to_string()\n_1984 = f['Word'].to_string()\n\nmydict1 = {\n    'HRC'  : [len(e['Word'])],\n    '1984' : [len(f['Word'])]}\n\ndf3 = pd.DataFrame(mydict1, index=['words'])\ndf3","execution_count":null,"outputs":[]},{"metadata":{},"id":"active-spring","cell_type":"markdown","source":"## Most Used Words"},{"metadata":{"trusted":true},"id":"institutional-divide","cell_type":"code","source":"colors = ['lightcoral', 'brown', 'firebrick', 'darkred', 'red', 'tomato', 'coral', 'orangered','sienna','sandybrown']\ncolors1 = ['navy', 'deepskyblue', 'teal','aqua', 'mediumblue', 'cadetblue', 'blue', 'mediumpurple', 'royalblue', 'dodgerblue']\n\nfig, axs = plt.subplots(1,2, figsize=(15,9))\nplt.rcParams.update({'font.size' : 20})\nfig.suptitle('Top 10 Used Words')\naxs[0].pie(a['Count'], labels=a['Word'], colors=colors)\naxs[1].pie(b['Count'], labels=b['Word'], colors=colors1)\naxs[0].set_title(\"HRC\")\naxs[1].set_title(\"1984\")\n\n ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Both sets of words are implicative of the role played by each group. \n\nHaving a high profile position of power can be seen by the common use of words like: government, relationships, support and leaders. \n\nMeanwhile, the main character of Orwell's novel used opposing words: ALWAYS and NEVER the exact same number of times. Possibly in resistance to the onslaught of doublespeak that plagues his world\n\nThe pie charts above as well as the pandas data frames below summarize the most frequent words used by each group in the study. \n\nInteresting note may be that the frequency count for the most common words used in 1984 is almost double than that of former presidential candidate. Possibly implying a more limited selection of words from the residents of the Orwellian Universe. "},{"metadata":{"trusted":true},"id":"reliable-officer","cell_type":"code","source":"a['Type'] = np.repeat('HRC', len(a['Word']))\nb['Type'] = np.repeat('1984', len(b['Word']))\n\na = a.set_index([np.arange(0,10,step=1)])\nb = b.set_index([np.arange(0,10,step=1)])\n\nab = pd.concat([y.reset_index(drop=True) for y in [a, b]], axis=1)\nab","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"electoral-comfort","cell_type":"code","source":"fig5, axs5 = plt.subplots(1,2, figsize=(15,9))\nplt.rcParams.update({'font.size' : 20})\nfig5.suptitle('Bottom 10 Used Words')\naxs5[1].pie(c['Count'], labels=c['Word'], colors=colors)\naxs5[0].pie(d['Count'], labels=d['Word'], colors=colors1)\naxs5[1].set_title(\"HRC\")\naxs5[0].set_title(\"1984\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"c['Type'] = np.repeat('HRC', len(a['Word']))\nd['Type'] = np.repeat('1984', len(b['Word']))\n\nc = c.set_index([np.arange(0,10,step=1)])\nd = d.set_index([np.arange(0,10,step=1)])\n\ncd = pd.concat([y.reset_index(drop=True) for y in [c, d]], axis=1)\ncd","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"nasty-mailing","cell_type":"code","source":"wordcloud = WordCloud(max_font_size = 80, background_color = 'white',\n                     collocations = True, colormap='magma').generate(hrc)\nplt.figure()\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.suptitle(\"HRC's Diction\")\n\nwordcloud1 = WordCloud(max_font_size = 80, background_color = \"white\", \n                      collocations = True, colormap = \"ocean\").generate(_1984)\nplt.figure()\nplt.imshow(wordcloud1)\nplt.axis(\"off\")\nplt.suptitle(\"1984's Diction\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Cool graphics depicting each group's trends"},{"metadata":{},"id":"sublime-program","cell_type":"markdown","source":"### Distribution of Word Use"},{"metadata":{"scrolled":true,"trusted":true},"id":"separated-reminder","cell_type":"code","source":"g = e.append(f, ignore_index=True)\nfac = sns.FacetGrid(g, col='Type', height=4.5, aspect=1.8)\nfac.map_dataframe(sns.histplot, x='Count', kde=True, binwidth=1)\nfac.set_axis_labels('Word Frequency', 'Count')\nfac.set(xticks=[x for x in np.arange(start=1, stop=11, step=1)])\nfac.fig.suptitle('How Often Was Each Word Used?')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Although the data was collected from different points in history and despite the fact that each group had a very different outlook towards the world around them, the word frequency distribution charts shows some interesting information. Although the individual words were different, the overall usage was very similar. With the same number of observations contributed by each group, we can see that about 150, out of 199 words(not including STOPWORDS) were used only once in the text. The words that were used twice were only used about 12-13% of the time and the progression falling off as a geometric distribution. "},{"metadata":{},"id":"finnish-conservative","cell_type":"markdown","source":"### Outliers"},{"metadata":{"trusted":true},"id":"behavioral-steering","cell_type":"code","source":"sns.set_theme(style='darkgrid')\nbox = sns.boxplot(x='Type', y='Count', data=g, hue='Type', \n                  palette='Set3').set_title('Boxplot of Word Usage')\nplt.yticks(np.arange(start=0, stop=8, step=2))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With 75% of the observations only ocurring once in each dataset, these boxplots show that even words that are only used twice are considered outlieres. "},{"metadata":{},"id":"liquid-network","cell_type":"markdown","source":"## Summary Statistics"},{"metadata":{"trusted":true},"id":"blond-chain","cell_type":"code","source":"var = 'Count'\ntype_grp = g.groupby('Type')\n\nxbar_hrc = type_grp.mean()[var].iloc[1]\nxbar_1984 = type_grp.mean()[var].iloc[0]\ns_hrc = type_grp.std()[var].iloc[1]\ns_1984 = type_grp.std()[var].iloc[0]\nn_hrc = type_grp.count()[var].iloc[1]\nn_1984 = type_grp.count()[var].iloc[0]\nvar_hrc = type_grp.var()[var].iloc[1]\nvar_1984 = type_grp.var()[var].iloc[0]\n\nmydict2 = {\n    'HRC'  : [xbar_hrc, s_hrc, n_hrc, var_hrc],\n    '1984' : [xbar_1984, s_1984, n_1984, var_1984]\n}\n\ndf4 = pd.DataFrame(mydict2, index=['mean', 'std', 'n', 'var']) \ndf5 = df4.round(decimals=3)\ndf5","execution_count":null,"outputs":[]},{"metadata":{},"id":"pretty-lawyer","cell_type":"markdown","source":"### t-Test to check for difference in mean from two independent distributions of word usage"},{"metadata":{"trusted":true},"id":"confidential-antique","cell_type":"code","source":"tobs = (xbar_hrc - xbar_1984) / ( s_hrc**2/n_hrc + s_1984**2/n_1984 )**(1/2)\ndeg_free = (s_hrc**2/n_hrc + s_1984**2/n_1984)**2 / ( (s_hrc**2/n_hrc)**2/(n_hrc-1) + (s_1984**2/n_1984)**2/(n_1984-1) ) \nt_dist = ss.t(deg_free)\npval = t_dist.cdf(tobs)\n\nmydict3 = {\n    't' : tobs,\n    'df' : deg_free,\n    'pval' : pval\n}\n\ndf6 = pd.DataFrame(mydict3, index=['Count'])\ndf6\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"uniform-chance","cell_type":"code","source":"# df6.append(df7).set_index([pd.Index(['Count', 'Length'])])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":5}
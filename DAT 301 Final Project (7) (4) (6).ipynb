{"cells":[{"metadata":{"trusted":true},"id":"brief-oxide","cell_type":"code","source":"# !pip install bs4\n# !pip install requests\n# !pip install pandas\n# !pip install wordcloud\n# ! pip install seaborn\n\n\nimport sys\nfrom os import path\nimport numpy as np\nimport requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup as bs\nfrom wordcloud import WordCloud, STOPWORDS\nimport re\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom random import sample\nimport scipy.stats as ss","execution_count":null,"outputs":[]},{"metadata":{},"id":"liquid-forest","cell_type":"markdown","source":"# Hillary Clinton"},{"metadata":{"trusted":true},"id":"demonstrated-thanksgiving","cell_type":"code","source":"url = \"https://wikileaks.org/clinton-emails/?q=%28favor+%26+confidential%29+%7C+%28favor+%26+classified%29&mfrom=&mto=&title=&notitle=&date_from=&date_to=&nofrom=&noto=&count=200&sort=0&page=1&#searchresult\"\nr = requests.get(url)\nsoup = bs(r.content, 'html.parser')","execution_count":null,"outputs":[]},{"metadata":{},"id":"inside-austin","cell_type":"markdown","source":"### Locate and Create Table of Target Emails"},{"metadata":{"trusted":true},"id":"tracked-reality","cell_type":"code","source":"table = soup.find(\"table\",  class_='table table-striped search-result')\ntab_rows = table.find_all('tr')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"adverse-geography","cell_type":"code","source":"l = []\nfor tr in tab_rows:\n    td = tr.find_all('td')\n    row = tr.get_text().strip() \n    l.append(row)","execution_count":null,"outputs":[]},{"metadata":{},"id":"collaborative-acoustic","cell_type":"markdown","source":"### Turn Table into Data Frame"},{"metadata":{"trusted":true},"id":"ultimate-sword","cell_type":"code","source":"df = pd.DataFrame(l)\ndf[['DocID', 'Date', 'Subject', 'From', 'To']] = df[0].str.split('\\n', expand=True)\ndf = df.iloc[1:,1:]","execution_count":null,"outputs":[]},{"metadata":{},"id":"rolled-enhancement","cell_type":"markdown","source":"### Create Giant Text List for HRC"},{"metadata":{"trusted":true},"id":"baking-shell","cell_type":"code","source":"url1 = 'https://wikileaks.org/clinton-emails/emailid/'\n\nli = []\nfor i in df.iloc[1:40,0]:\n    one = url1 + i\n    one1 = requests.get(one)\n    two = bs(one1.content, 'html.parser')\n    three = two.find(id='uniquer')\n    four = three.get_text().strip()\n    li.append(four)","execution_count":null,"outputs":[]},{"metadata":{},"id":"offshore-cleaning","cell_type":"markdown","source":"### Create Giant Text List for 1984"},{"metadata":{"trusted":true},"id":"nutritional-command","cell_type":"code","source":"url2 = 'http://www.george-orwell.org/1984/'\n\nstory = []\nfor i in range(22,23):\n    myurl = url2 + str(i) + '.html'\n    s = requests.get(myurl)\n    ch = bs(s.content, 'html.parser')\n    six = [sib.get_text() for sib in ch.find('h2').next_siblings]\n    story.append(six)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"after-attention","cell_type":"code","source":"eight = [str(i).split() for i in story]\nnine = []\nfor phrase in eight:\n    for word in phrase:\n        nine.append(word)\n","execution_count":null,"outputs":[]},{"metadata":{},"id":"eastern-formation","cell_type":"markdown","source":"### Create Word Usage Data Frame for 1984"},{"metadata":{"trusted":true},"id":"cooked-degree","cell_type":"code","source":"words2 = [ str(w).lower().strip() for w in nine ]\ntable = str.maketrans(dict.fromkeys(string.punctuation))\nwords1 = [i.translate(table) for i in nine]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"veterinary-hanging","cell_type":"code","source":"freq1 = [words1.count(w) for w in words1]\nsev = dict(list(zip(words1, freq1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"close-guarantee","cell_type":"code","source":"usage1 = pd.DataFrame(list(sev.items()),columns=['Word', 'Count'])\nusage1.sort_values(by='Count', ascending=False, inplace=True)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"knowing-november","cell_type":"code","source":"\n","execution_count":null,"outputs":[]},{"metadata":{},"id":"extraordinary-muscle","cell_type":"markdown","source":"### Create Word Usage Data Frame for HRC"},{"metadata":{"trusted":true},"id":"organic-consistency","cell_type":"code","source":"wordlist = list(re.sub('[' + string.punctuation + ']', '', four).lower().split())   \nfreq = [wordlist.count(w) for w in wordlist]\nfive = dict(list(zip(wordlist, freq)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"eastern-evans","cell_type":"code","source":"usage = pd.DataFrame(list(five.items()),columns=['Word', 'Count'])\nusage.sort_values(by='Count', ascending=False, inplace=True)\n\n","execution_count":null,"outputs":[]},{"metadata":{},"id":"decreased-salmon","cell_type":"markdown","source":"### STOPWORDS and Other Filters"},{"metadata":{"trusted":true},"id":"hawaiian-encoding","cell_type":"code","source":"stopwords = []\nstopwords = list(set(STOPWORDS))\nstopwords += ['>','from:','to:', 'no.', 'date:','sent:','subject:', 're:',\n             'original', 'message', 'cameron', 'robinson', 'shaun',\n             'c05774510', '11302015', 'f201420439', \"o\\\\'brien\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"desirable-community","cell_type":"code","source":"filt = (usage.Word.isin(stopwords))\nfilt1 = (usage1.Word.isin(stopwords))\nunique = usage[~filt]\nunique1 = usage1[~filt1]\nlength = [len(w) for w in unique.Word]\nlength1 = [len(w) for w in unique1.Word]\nunique.insert(2, 'Length', length, True)\nunique1.insert(2, 'Length', length1, True)\nfilt2 = (unique['Length'] > 4)\nfilt3 = (unique1['Length'] > 4)\n#---------------------------------------------------\nusage2 = usage.sort_values(by='Count', ascending=True)\nusage3 = usage1.sort_values(by='Count', ascending=True)\nfilt6 = (usage2.Word.isin(stopwords))\nfilt7 = (usage3.Word.isin(stopwords))\nunique2 = usage2[~filt6]\nunique3 = usage3[~filt7]\n\nlength2 = [len(w) for w in unique2.Word]\nlength3 = [len(w) for w in unique3.Word]\nunique2.insert(2, 'Length', length, True)\nunique3.insert(2, 'Length', length1, True)\nfilt4 = (unique2['Length'] > 4)\nfilt5 = (unique3['Length'] > 4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Top and Bottom 10 Used Words"},{"metadata":{"trusted":true},"id":"excess-sewing","cell_type":"code","source":"a=unique[filt2].iloc[1:11,0:2]\nb=unique1[filt3].iloc[1:11,0:2]\n#--------------\nc=unique2[filt4].iloc[1:11,0:2]\nd=unique3[filt5].iloc[1:11,0:2]\n\nhrc_lst = unique[filt2]\n_1984_lst = unique1[filt3]\n\nmydict = {\n    'HRC'  : [len(hrc_lst['Word'])],\n    '1984' : [len(_1984_lst['Word'])]}\n\ndf2 = pd.DataFrame(mydict, index=['words'])\ndf2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"advanced-array","cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"severe-gamma","cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"nutritional-fields","cell_type":"code","source":"e = unique[filt2]#.iloc[1:101,0:3]\n\ne['Type'] = np.repeat('HRC', len(e['Word']))\nff = unique1[filt3]#.iloc[1:101,0:3]\nff['Type'] = np.repeat(1984, len(ff['Word']))\n\nfff = sample(list(np.arange(0,len(ff['Word']), step=1)), len(e['Word']))\n\nf = ff.iloc[fff]\n\n#--------------------------------\n\nhrc = e['Word'].to_string()\n_1984 = f['Word'].to_string()\n\n# hrc = [word for word in e['Word']].to_string()\n# _1984 = [word for word in f['Word1']]\n\nmydict1 = {\n    'HRC'  : [len(e['Word'])],\n    '1984' : [len(f['Word'])]}\n\ndf3 = pd.DataFrame(mydict1, index=['words'])\ndf3\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"colors = ['lightcoral', 'brown', 'firebrick', 'darkred', 'red', 'tomato', 'coral', 'orangered','sienna','sandybrown']\ncolors1 = ['navy', 'deepskyblue', 'teal','aqua', 'mediumblue', 'cadetblue', 'blue', 'mediumpurple', 'royalblue', 'dodgerblue']\n\nfig, axs = plt.subplots(1,2, figsize=(15,9))\nplt.rcParams.update({'font.size' : 18})\nfig.suptitle('Top 10 Used Words')\naxs[0].pie(e['Count'], labels=e['Word'], colors=colors)\naxs[1].pie(f['Count'], labels=f['Word'], colors=colors1)\naxs[0].set_title(\"HRC's\")\naxs[1].set_title(\"1984 Pt. 2/3\")\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"wordcloud = WordCloud(max_font_size = 80, background_color = 'white',\n                     collocations = True, colormap='magma').generate(hrc)\nplt.figure()\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.suptitle(\"HRC's Diction\")\n\nwordcloud1 = WordCloud(max_font_size = 80, background_color = \"white\", \n                      collocations = True, colormap = \"ocean\").generate(_1984)\nplt.figure()\nplt.imshow(wordcloud1)\nplt.axis(\"off\")\nplt.suptitle(\"1984's Diction\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"separated-reminder","cell_type":"code","source":"g = e.append(f, ignore_index=True)\nfac = sns.FacetGrid(g, col='Type', height=5.5, aspect=1.2)\nfac.map_dataframe(sns.histplot, x='Count', kde=True, binwidth=1)\nfac.set_axis_labels('Word Frequency', 'Count')\nfac.set(xticks=[x for x in np.arange(start=0, stop=24, step=2)])\nfac.fig.suptitle('How Often Was Each Word Used?')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"id":"behavioral-steering","cell_type":"code","source":"sns.set_theme(style='darkgrid')\nbox = sns.boxplot(x='Type', y='Count', data=g, hue='Type', \n                  palette='Set3').set_title('Boxplot of Word Usage')\nplt.yticks(np.arange(start=0, stop=8, step=2))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'Count'\ntype_grp = g.groupby('Type')\n\nxbar_hrc = type_grp.mean()[var].iloc[1]\nxbar_1984 = type_grp.mean()[var].iloc[0]\ns_hrc = type_grp.std()[var].iloc[1]\ns_1984 = type_grp.std()[var].iloc[0]\nn_hrc = type_grp.count()[var].iloc[1]\nn_1984 = type_grp.count()[var].iloc[0]\nvar_hrc = type_grp.var()[var].iloc[1]\nvar_1984 = type_grp.var()[var].iloc[0]\n\nmydict2 = {\n    'HRC'  : [xbar_hrc, s_hrc, n_hrc, var_hrc],\n    '1984' : [xbar_1984, s_1984, n_1984, var_1984]\n}\n\ndf4 = pd.DataFrame(mydict2, index=['mean', 'std', 'n', 'var']) \ndf5 = df4.round(decimals=3)\ndf5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tobs = (xbar_hrc - xbar_1984) / ( s_hrc**2/n_hrc + s_1984**2/n_1984 )**(1/2)\ndeg_free = (s_hrc**2/n_hrc + s_1984**2/n_1984)**2 / ( (s_hrc**2/n_hrc)**2/(n_hrc-1) + (s_1984**2/n_1984)**2/(n_1984-1) ) \nt_dist = ss.t(deg_free)\npval = t_dist.cdf(tobs)\n\nmydict3 = {\n    't' : tobs,\n    'df' : deg_free,\n    'pval' : pval\n}\n\ndf6 = pd.DataFrame(mydict3, index=['Count'])\ndf6\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# df6.append(df7).set_index([pd.Index(['Count', 'Length'])])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":5}